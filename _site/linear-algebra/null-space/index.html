<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.1 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Understanding the Null Space ‚Äì From Geometry to ML Applications - Intuitive Maths</title>
<meta name="description" content="A website to learn maths intuitively">


  <meta name="author" content="Chaitanya Agrawal">
  


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Intuitive Maths">
<meta property="og:title" content="Understanding the Null Space ‚Äì From Geometry to ML Applications">
<meta property="og:url" content="http://localhost:4000/linear-algebra/null-space/">


  <meta property="og:description" content="A website to learn maths intuitively">











  

  


<link rel="canonical" href="http://localhost:4000/linear-algebra/null-space/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Intuitive Maths Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Intuitive Maths
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/"
                
                
              >Quick-Start Guide</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      packages: ['base', 'ams']
    },
    loader: {
      load: ['[tex]/ams']
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>







  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/" itemprop="item"><span itemprop="name">Home</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">/</span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/linear-algebra" itemprop="item"><span itemprop="name">Linear algebra</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">/</span>
      
    
      
      
        <li class="current">Understanding the Null Space ‚Äì From Geometry to ML Applications</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Understanding the Null Space ‚Äì From Geometry to ML Applications">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="http://localhost:4000/linear-algebra/null-space/" itemprop="url">Understanding the Null Space ‚Äì From Geometry to ML Applications
</a>
          </h1>
          


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#lets-start-with-a-question">Let‚Äôs Start with a Question!</a></li><li><a href="#too-many-solutions-welcome-to-the-null-space">Too Many Solutions? Welcome to the Null Space</a></li><li><a href="#the-black-hole-of-matrices-what-gets-pulled-in-and-lost">The Black Hole of Matrices: What Gets Pulled In and Lost</a></li><li><a href="#what-exactly-is-the-null-space">What Exactly Is the Null Space?</a></li><li><a href="#zooming-into-the-math">Zooming into the Math</a><ul><li><a href="#null-space-in-linear-regression">Null Space in Linear Regression</a></li><li><a href="#null-space-of-projection-matrix">Null Space of Projection Matrix</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <h2 id="lets-start-with-a-question">Let‚Äôs Start with a Question!</h2>

<ul>
  <li>What does it really mean for a matrix to ‚Äúsend‚Äù a vector to zero?</li>
  <li>Why should an ML engineer or a data scientist care about such a concept?</li>
</ul>

<p>If you‚Äôre diving into machine learning, understanding the <strong>null space</strong> is more than a textbook exercise. It‚Äôs key to understanding how models behave under the hood.</p>

<h2 id="too-many-solutions-welcome-to-the-null-space">Too Many Solutions? Welcome to the Null Space</h2>

<p>Let‚Äôs say you‚Äôre building a regression model to predict house prices and you are using the following features to train the model:</p>
<ul>
  <li><b>Feature 1</b> : Area of the house</li>
  <li><b>Feature 2</b> : Number of rooms</li>
  <li><b>Feature 3</b> : 2 * (Area of the house)</li>
</ul>

<p>You fit your model, get great accuracy and then realize: there are infinitely many sets of weights that yield the exact same predictions.</p>

<p>What‚Äôs going on here?</p>

<p>This is where the null space enters the picture. It represents the ‚Äúinvisible directions‚Äù (vectors) in your feature space, i.e., changes that don‚Äôt affect the output at all. In our example, this has happened because of the presence of multicollinearity. <br />
<a href="#null-space-in-linear-regression">Check the maths below</a></p>

<h2 id="the-black-hole-of-matrices-what-gets-pulled-in-and-lost">The Black Hole of Matrices: What Gets Pulled In and Lost</h2>
<p>Let‚Äôs build some intuition before jumping into the math.</p>

<p>When we project a 3D object onto a 2D plane, all the information about the object‚Äôs depth (the component perpendicular to the plane) is lost and maps to zero in that dimension on the 2D plane. The vectors representing that ‚Äúdepth-only‚Äù information would be in the null space of the projection transformation.</p>

<p>A matrix transformation does something similar: it takes a vector and transforms it, but certain directions get completely wiped out. Those are the directions that belong to the null space.</p>

<p>So when we say a vector lies in the null space of a matrix $A$, we mean that applying $A$ to that vector turns it into the zero vector. <br />
<a href="#null-space-of-projection-matrix">Check the math below</a></p>

<h2 id="what-exactly-is-the-null-space">What Exactly Is the Null Space?</h2>

<p>Given matrix $A \in \mathbb{R}^{m \times n}$, we define the null space of $A$ as:</p>

\[\mathcal{N}(A) = \{ x \in \mathbb{R}^n \mid Ax = 0 \}\]

<p>In simple terms:</p>
<ul>
  <li>It‚Äôs the set of all input vectors ùë• that the matrix ‚Äúkills‚Äù ‚Äî sends to the origin.</li>
  <li>It always forms a subspace of $\mathbb{R}^n$</li>
  <li>Its dimension tells you how many directions are ‚Äúinvisible‚Äù to your matrix.</li>
</ul>

<blockquote>
  <p>If the null space contains more than just the zero vector, your matrix is not full-rank, and your system of equations (or ML model) has multiple solutions.</p>
</blockquote>

<p>Let‚Äôs look at $N(A)$ from another perspective. We can represent its rows as row vectors:</p>

\[A = \begin{pmatrix}
a^T_{1} \\
a^T_{2} \\
a^T_{3} \\
\vdots \\
a^T_{m}
\end{pmatrix}\]

<p>where $a^T_i$ is the transpose of the $i^{\text{th}}$ row vector ($a_i$ is a column vector in $\mathbb{R}^n$).<br />
Now, when we compute $Ax$, we are essentially taking the dot product of each row of $A$ with the vector $x$:</p>

\[Ax = \begin{pmatrix}
a^T_{1} x \\
a^T_{2} x \\
a^T_{3} x \\
\vdots \\
a^T_{m} x
\end{pmatrix}
= 
\begin{pmatrix}
a_{1} \cdot x \\
a_{2} \cdot x \\
a_{3} \cdot x \\
\vdots \\
a_{m} \cdot x
\end{pmatrix}\]

<p>For $x$ to be in the $N(A)$, we must have $Ax = 0$. This means:</p>

\[\begin{pmatrix}
a_{1} \cdot x \\
a_{2} \cdot x \\
a_{3} \cdot x \\
\vdots \\
a_{m} \cdot x
\end{pmatrix}
=
\begin{pmatrix}
0 \\
0 \\
0 \\
\vdots \\
0
\end{pmatrix}\]

<p>This set of equations implies that for every row vector $a_i$ of $A$, the dot product with $x$ must be zero:</p>

\[a_i \cdot x = 0 \quad \text{for all } i = 1, 2, 3, \dots, m\]

<p>Since we know that the dot product of two vectors being zero means the vectors are orthogonal to each other,<br />
<strong>a vector $x$ is in the null space of $A$ if and only if $x$ is orthogonal to every row vector of matrix $A$.</strong></p>

<blockquote>
  <p>The row space of $A$, denoted as $\text{Row}(A)$, is the span of the row vectors of $A$. Since $x$ is orthogonal to each row vector, it must also be orthogonal to any linear combination of the row vectors. This means that $N(A)$ is orthogonal to the entire row space of $A$. We can write this as: <br />
\begin{gathered} 
 \mathcal{N}(A) \perp \text{Row}(A) 
\end{gathered}</p>
</blockquote>

<h2 id="zooming-into-the-math">Zooming into the Math</h2>

<h3 id="null-space-in-linear-regression">Null Space in Linear Regression</h3>

<p>Let‚Äôs say you‚Äôre building a regression model. What if there are multiple equally good solutions?
This usually means your data matrix has a null space.</p>

<p>Now, let‚Äôs look at the equation to solve for ùõΩ:</p>

\[y=X\beta+ \epsilon\]

<p>where, <br />
X: features matrix <br />
Œ≤: vector of coefficients you want to learn <br />
y: obutput/target vector <br />
œµ: Noise <br /></p>

<p>Now imagine that there is not just one ùõΩ that gives you a good fit, but infinitely many.
Why? Because the system has redundant information, and part of ùõΩ can move freely without changing the outcome ùë¶. This ‚Äúfreedom‚Äù is precisely what the null space captures.</p>

<p>The null space of ùëã is the set of all vectors ùë£ such that:</p>

\[ùëãùë£=0\]

<p>If ùë£ is in the null space, and you have any solution $ùõΩ_{0}$, then $ùõΩ_0+ùë£$ is also a solution. Why? Because:</p>

\[XŒ≤=X(ùõΩ_0+ùë£) = XùõΩ_0 + Xùë£ = y+0\]

<p>This means the null space contains the ‚Äúdirections‚Äù along which we can move without affecting predictions.</p>

<blockquote>
  <p>If the null space of features matrix is non-trivial (i.e., not just the zero vector), then there is <strong>collinearity</strong> (linearly dependent features).</p>
</blockquote>

<p>Now, let‚Äôs use the house price prediction example discussed above to see how we can calculate those ‚Äúinvisible directions‚Äù, along which changing the coefficients will not affect predictions. First, let‚Äôs create a matrix using mock data:</p>

<ul>
  <li><b>Feature 1</b> : Area of the house</li>
  <li><b>Feature 2</b> : Number of rooms</li>
  <li><b>Feature 3</b> : 2 * (Area of the house)</li>
</ul>

\[X = \begin{bmatrix}
100 &amp; 2 &amp; 200 \\
300 &amp; 4 &amp; 600 \\
150 &amp; 1 &amp; 300 \\
450 &amp; 3 &amp; 900
\end{bmatrix}\]

<p>For the unique solution of the linear regression to exist, the Gram matrix $X^TX$ in the Normal Equation ($ \hat\beta = (X^TX)^{-1}.(X^Ty)$) should be invertible.
But due to presence of collinearity in the example above, the Gram matrix $X^TX$ is clealy not invertible, which implies multiple solutions exist for $X\beta=y$. <strong>Since, $\beta = \beta_0 + \nu$,
where $\nu \in \mathcal{N}(X)$, any two solutions differ by a vector in the null space of $X$</strong>.</p>

<p>Let‚Äôs solve the following for $\nu$, to find the null space of $X$:</p>

\[X\nu=0\]

<p>i.e., get <strong>non-zero</strong> coefficient directions invisible to the data; moving along those directions does not change predictions.</p>

\[\mathcal{N}(X) = \left\{ t\begin{bmatrix}
      -2\\
      0\\
      1
      \end{bmatrix} \bigg | t \in \mathbb{R} \right\}\]

<p>It means any coefficient vector differing by a multiple of $\begin{bmatrix} -2 &amp; 0 &amp; 1 \end{bmatrix}^T$ produces identical predictions.</p>

<p><strong>Important points to note:</strong></p>
<ul>
  <li>
    <p>For a linear regression features matrix $X \in \mathbb{R}^{n \times p} $:</p>

    <center>
$$
\mathcal{N}(X) \neq \{0\} \iff \text{columns of $X$ are linearly dependent}
$$ 
</center>

    <p>And linear dependence of features is exactly what multicollinearity means.
So in regression analysis:</p>
    <blockquote>
      <p>A non-trivial null space of $X$ implies perfect multicollinearity among the features.</p>
    </blockquote>
  </li>
  <li>
    <p>High correlation $\Leftrightarrow$ numerical instability (not null space). <br />
It means columns of $X$ are nearly linearly dependent but no exact linear combination equals zero. <br />
So: <br /></p>
    <center>
$$
   \mathcal{N}(X) = \{0\}
$$
</center>

    <p>For a highly correlated features matrix (feature 1 $\approx$ feature 2), the regression coefficients are highly sensitive to noise and rounding erros in the features. <br />
The model struggles to decide how to split weight between them. Consequently, even small noise can drastically flip the magnitude or reverse the signs of the coefficients.</p>
  </li>
</ul>

<h3 id="null-space-of-projection-matrix">Null Space of Projection Matrix</h3>

<p>As discussed in the example above, let‚Äôs consider a simple projection from 3D to 2D.</p>

\[T:\mathbb{R}^3 \to \mathbb{R}^2\]

<p>For example, projecting 3D object onto the $xy$-plane:</p>

\[T(x,y,z) = (x,y)\]

<p>This transformation <strong>keeps</strong> the $x$ and $y$ components and <b>drops</b> the $z$ component.
Any point with different $z$-values but the same $x,y$ will map to the same point in 2D:</p>

\[(1, 2, 5) \mapsto (1,2)\]

\[(1, 2, -7) \mapsto (1,2)\]

<p>All depth information (the $z$-direction) is destroyed by the projection. <br />
Now, this projection matrix, given by the linear transformation $T$, can be written as:</p>

\[T = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0
\end{bmatrix}\]

<p>This matrix:</p>
<ul>
  <li>keeps the $x$ and $y$ components</li>
  <li>discard the $z$ component entirely</li>
</ul>

<p><br />
Applying it to a vector $(x,y,z)$:</p>

\[T\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}= \begin{bmatrix} x \\ y \end{bmatrix}\]

<p>Let‚Äôs see which vectors get mapped to zero, i.e., the directions that disappear when we project a 3D object onto a 2D plane.</p>

<p>Solve:</p>

\[T\nu=0\]

\[\begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0
\end{bmatrix}

\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}=
\begin{bmatrix}
0 \\
0 \\
\end{bmatrix}\]

<p>This implies:</p>
<ul>
  <li>$x=0$</li>
  <li>$y=0$</li>
  <li>$z$ can be <em>anything</em></li>
</ul>

<p>So the null space is:</p>

\[\mathcal{N}(T) = \left\{ \begin{bmatrix}
      0\\
      0\\
      z
      \end{bmatrix} \,\bigg|\, z \in \mathbb{R} \right\}\]

<p>Here, key interpretation is that any moveement purely along the $z$-axis produce no change at all in the projected 2D output. Therefore, $z$-direction is invisible to the matrix $T$.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The null space reveals the hidden redundancies in your data‚Äîthe directions along which change produces no effect. Whether it‚Äôs multicollinearity in regression, information lost in projection, or multiple solutions to a system, understanding the null space tells you where your matrix is ‚Äúblind.‚Äù For practitioners, a non-trivial null space is a red flag: it signals dependent features, rank deficiency, and the need to either remove redundant information or embrace the freedom of multiple solutions. By recognizing these invisible directions, you gain the insight to build more robust models and interpret their limitations.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;script src="https://utteranc.es/client.js"
        repo="https://github.com/Agrawal-Chaitanya/agrawal-chaitanya.github.io"
        issue-term="pathname"
        label="Comment"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async&gt;
&lt;/script&gt;
</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#linear-algebra" class="page__taxonomy-item p-category" rel="tag">Linear Algebra</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#mathematics" class="page__taxonomy-item p-category" rel="tag">Mathematics</a>
    
    </span>
  </p>


        

      </footer>

      

      

    </div>

    
  </article>

  
  
</div>
      
    </div>

    <script src="https://utteranc.es/client.js"
        repo="https://github.com/Agrawal-Chaitanya/agrawal-chaitanya.github.io"
        issue-term="pathname"
        label="Comment"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>
    </script>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2026 <a href="http://localhost:4000">Intuitive Maths</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>









  </body>
</html>

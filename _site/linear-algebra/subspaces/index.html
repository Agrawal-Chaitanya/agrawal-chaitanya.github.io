<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.1 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Understanding the Null Space â€“ From Geometry to ML Applications - Intuitive Maths</title>
<meta name="description" content="A website to learn maths intuitively">


  <meta name="author" content="Chaitanya Agrawal">
  


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Intuitive Maths">
<meta property="og:title" content="Understanding the Null Space â€“ From Geometry to ML Applications">
<meta property="og:url" content="http://localhost:4000/linear-algebra/subspaces/">


  <meta property="og:description" content="A website to learn maths intuitively">











  

  


<link rel="canonical" href="http://localhost:4000/linear-algebra/subspaces/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Intuitive Maths Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Intuitive Maths
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/"
                
                
              >Quick-Start Guide</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      packages: ['base', 'ams']
    },
    loader: {
      load: ['[tex]/ams']
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>








<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Understanding the Null Space â€“ From Geometry to ML Applications">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="http://localhost:4000/linear-algebra/subspaces/" itemprop="url">Understanding the Null Space â€“ From Geometry to ML Applications
</a>
          </h1>
          


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#lets-start-with-a-question">Letâ€™s Start with a Question!</a></li><li><a href="#too-many-solutions-welcome-to-the-null-space">Too Many Solutions? Welcome to the Null Space</a></li><li><a href="#the-black-hole-of-matrices-what-gets-pulled-in-and-lost">The Black Hole of Matrices: What Gets Pulled In and Lost</a></li><li><a href="#what-exactly-is-the-null-space">What Exactly Is the Null Space?</a></li><li><a href="#zooming-into-the-math">Zooming into the Math</a><ul><li><a href="#null-space-in-linear-regression">Null Space in Linear Regression</a></li></ul></li></ul>
            </nav>
          </aside>
        
        <h2 id="lets-start-with-a-question">Letâ€™s Start with a Question!</h2>

<ul>
  <li>What does it really mean for a matrix to â€œsendâ€ a vector to zero?</li>
  <li>Why should an ML engineer or a data scientist care about such a concept?</li>
</ul>

<p>If youâ€™re diving into machine learning, understanding the <strong>null space</strong> is more than a textbook exercise. Itâ€™s key to understanding how models behave under the hood.</p>

<h2 id="too-many-solutions-welcome-to-the-null-space">Too Many Solutions? Welcome to the Null Space</h2>

<p>Letâ€™s say youâ€™re building a regression model to predict house prices and you are using the following features to train the model:</p>
<ul>
  <li><b>Feature 1</b> : Area of the house</li>
  <li><b>Feature 2</b> : Number of rooms</li>
  <li><b>Feature 3</b> : 2 * (Area of the house)</li>
</ul>

<p>You fit your model, get great accuracy and then realize: there are infinitely many sets of weights that yield the exact same predictions.</p>

<p>Whatâ€™s going on here?</p>

<p>This is where the null space enters the picture. It represents the â€œinvisible directionsâ€ (vectors) in your feature space, i.e., changes that donâ€™t affect the output at all. In our example, this has happened because of the presence of multicollinearity.</p>

<h2 id="the-black-hole-of-matrices-what-gets-pulled-in-and-lost">The Black Hole of Matrices: What Gets Pulled In and Lost</h2>
<p>Letâ€™s build some intuition before jumping into the math.</p>

<p>When we project a 3D object onto a 2D plane, all the information about the objectâ€™s depth (the component perpendicular to the plane) is lost and maps to zero in that dimension on the 2D plane. The vectors representing that â€œdepth-onlyâ€ information would be in the null space of the projection transformation.</p>

<p>A matrix transformation does something similar: it takes a vector and transforms it, but certain directions get completely wiped out. Those are the directions that belong to the null space.</p>

<p>So when we say a vector lies in the null space of a matrix $A$, we mean that applying $A$ to that vector turns it into the zero vector.</p>

<h2 id="what-exactly-is-the-null-space">What Exactly Is the Null Space?</h2>

<p>Given matrix $A \in \mathbb{R}^{m \times n}$, we define the null space of $A$ as:</p>

\[\mathcal{N}(A) = \{ x \in \mathbb{R}^n \mid Ax = 0 \}\]

<p>In simple terms:</p>
<ul>
  <li>Itâ€™s the set of all input vectors ğ‘¥ that the matrix â€œkillsâ€ â€” sends to the origin.</li>
  <li>It always forms a subspace of $\mathbb{R}^n$</li>
  <li>Its dimension tells you how many directions are â€œinvisibleâ€ to your matrix. (Explained in detail below - https://chatgpt.com/share/694d0338-dff4-800b-91d8-c591c33f5324)</li>
</ul>

<blockquote>
  <p>If the null space contains more than just the zero vector, your matrix is not full-rank, and your system of equations (or ML model) has multiple solutions.</p>
</blockquote>

<p>Letâ€™s look at $N(A)$ from another perspective. We can represent its rows as row vectors:</p>

\[A = \begin{pmatrix}
a^T_{1} \\
a^T_{2} \\
a^T_{3} \\
\vdots \\
a^T_{m}
\end{pmatrix}\]

<p>where $a^T_i$ is the transpose of the $i^{\text{th}}$ row vector ($a_i$ is a column vector in $\mathbb{R}^n$).<br />
Now, when we compute $Ax$, we are essentially taking the dot product of each row of $A$ with the vector $x$:</p>

\[Ax = \begin{pmatrix}
a^T_{1} x \\
a^T_{2} x \\
a^T_{3} x \\
\vdots \\
a^T_{m} x
\end{pmatrix}
= 
\begin{pmatrix}
a_{1} \cdot x \\
a_{2} \cdot x \\
a_{3} \cdot x \\
\vdots \\
a_{m} \cdot x
\end{pmatrix}\]

<p>For $x$ to be in the $N(A)$, we must have $Ax = 0$. This means:</p>

\[\begin{pmatrix}
a_{1} \cdot x \\
a_{2} \cdot x \\
a_{3} \cdot x \\
\vdots \\
a_{m} \cdot x
\end{pmatrix}
=
\begin{pmatrix}
0 \\
0 \\
0 \\
\vdots \\
0
\end{pmatrix}\]

<p>This set of equations implies that for every row vector $a_i$ of $A$, the dot product with $x$ must be zero:</p>

\[a_i \cdot x = 0 \quad \text{for all } i = 1, 2, 3, \dots, m\]

<p>Since we know that the dot product of two vectors being zero means the vectors are orthogonal to each other,<br />
<strong>a vector $x$ is in the null space of $A$ if and only if $x$ is orthogonal to every row vector of matrix $A$.</strong></p>

<blockquote>
  <p>The row space of $A$, denoted as $\text{Row}(A)$, is the span of the row vectors of $A$. Since $x$ is orthogonal to each row vector, it must also be orthogonal to any linear combination of the row vectors. This means that $N(A)$ is orthogonal to the entire row space of $A$. We can write this as: <br />
\begin{gathered} 
 N(A) \perp \text{Row}(A) 
\end{gathered}</p>
</blockquote>

<p><b> Make the illustration of null space using following regression example as sub heading of â€œZooming into the mathâ€ </b></p>
<h2 id="zooming-into-the-math">Zooming into the Math</h2>

<h3 id="null-space-in-linear-regression">Null Space in Linear Regression</h3>

<p>Letâ€™s say youâ€™re building a regression model. What if there are multiple equally good solutions?
This usually means your data matrix has a null space.</p>

<p>Now, letâ€™s look at the equation to solve for ğ›½:</p>

\[y=X\beta+ \epsilon\]

<p>where, <br />
X: design matrix (e.g., features) <br />
Î²: vector of coefficients you want to learn <br />
y: obutput/target vector <br />
Ïµ: Noise <br /></p>

<p>Now imagine that there is not just one ğ›½ that gives you a good fit, but infinitely many.
Why? Because the system has redundant information, and part of ğ›½ can move freely without changing the outcome ğ‘¦. This â€œfreedomâ€ is precisely what the null space captures.</p>

<p>The null space of ğ‘‹ is the set of all vectors ğ‘£ such that:</p>

\[ğ‘‹ğ‘£=0\]

<p>If ğ‘£ is in the null space, and you have any solution $ğ›½_{0}$, then $ğ›½_0+ğ‘£$ is also a solution. Why? Because:</p>

\[XÎ²=X(ğ›½_0+ğ‘£) = Xğ›½_0 + Xğ‘£ = y+0\]

<p>This means the null space contains the â€œdirectionsâ€ along which we can move without affecting predictions.</p>

<p>The null space contains non-identifiable components of the model, parts of the coefficient vector that you cannot learn uniquely from data.</p>

<p>If the null space is non-trivial (i.e., not just the zero vector), then there is <strong>collinearity</strong> (linearly dependent features).</p>

<p>Going back to the house price prediction example discussed in the starting of this post, letâ€™s create a matrix using mock data:</p>

<ul>
  <li><b>Feature 1</b> : Area of the house</li>
  <li><b>Feature 2</b> : Number of rooms</li>
  <li><b>Feature 3</b> : 2 * (Area of the house)</li>
</ul>

\[X = \begin{bmatrix}
100 &amp; 2 &amp; 200 \\
300 &amp; 4 &amp; 600 \\
150 &amp; 1 &amp; 300 \\
450 &amp; 3 &amp; 900
\end{bmatrix}\]

<p>For the unique solution of the linear regression to exist, the Gram matrix $X^TX$ in the Normal Equation ($ \hat\beta = (X^TX)^{-1}.(X^Ty)$) should be invertible.
But due to presence of collinearity in the example above, the Gram matrix $X^TX$ is clealy not invertible, which implies multiple solutions exist for $X\beta=y$.</p>

<p>Also, by explicitly solving,</p>

\[X\nu=0\]

<p>we can get <strong>non-zero</strong> coefficient directions invisible to the data; moving along those directions does not change predictions.</p>

\[\nu = t\begin{bmatrix}
      -2\\
      0\\
      1
      \end{bmatrix}, t \in \mathbb{R}\]

\[\mathcal{N}(X) = \left\{ t\begin{bmatrix}
      -2\\
      0\\
      1
      \end{bmatrix} \bigg| t \in \mathbb{R} \right\}\]

<p>â€“</p>

<p>=&gt; Write appendix to explain above points like proving how matrix transformation in the case of 3D to 2D projection works: https://chatgpt.com/share/6940646c-1680-800b-b75b-608f4b186272 
explain - https://chatgpt.com/share/694d0338-dff4-800b-91d8-c591c33f5324</p>

<p>proof: please elaborate in detail - Letâ€™s say youâ€™re building a regression model. What if there are multiple equally good solutions?<br />
This usually means your data matrix has a <strong>null space</strong>â€”a subspace where changes in input donâ€™t affect the output.</p>

<p>Suppose youâ€™re solving a linear regression problem:</p>

<p>ğ‘‹
ğ›½
=
ğ‘¦
XÎ²=y
ğ‘‹
X is your design matrix (e.g., features).</p>

<p>ğ›½
Î² is the vector of coefficients you want to learn.</p>

<p>ğ‘¦
y is your output/target vector.</p>

<p>Now imagine that there is not just one 
ğ›½
Î² that gives you a good fit â€” but infinitely many.</p>

<p>Why? Because the system has redundant information, and part of 
ğ›½
Î² can move freely without changing the outcome 
ğ‘¦
y.</p>

<p>This â€œfreedomâ€ is precisely what the null space captures.</p>

<p>ğŸ“ Geometric View
The null space of ğ‘‹ is the set of all vectors ğ‘£ such that:</p>

<p>ğ‘‹
ğ‘£
=
0
Xv=0
If 
ğ‘£
v is in the null space, and you have any solution 
ğ›½
0
Î² 
0
â€‹
 , then:</p>

<p>ğ›½
0
+
ğ‘£
Î² 
0
â€‹
 +v
is also a solution! Why?</p>

<p>Because:</p>

<p>ğ‘‹
(
ğ›½
0
+
ğ‘£
)
=
ğ‘‹
ğ›½
0
+
ğ‘‹
ğ‘£
=
ğ‘¦
+
0
=
ğ‘¦
X(Î² 
0
â€‹
 +v)=XÎ² 
0
â€‹
 +Xv=y+0=y
â¡ï¸ This means the null space contains the â€œdirectionsâ€ along which we can move without affecting predictions.</p>

<p>ğŸ§ª ML Interpretation
In practical ML:</p>

<p>The null space contains non-identifiable components of the model â€” parts of the coefficient vector that you cannot learn uniquely from data.</p>

<p>If the null space is non-trivial (i.e., not just the zero vector), then:</p>

<p>There is collinearity (linearly dependent features).</p>

<p>The matrix 
ğ‘‹
ğ‘‡
ğ‘‹
X 
T
 X is not invertible, so the normal equation doesnâ€™t have a unique solution.
<strong>Example</strong>:</p>

\[Ax =
\begin{bmatrix}
1 &amp; 1 &amp; 2 \\
2 &amp; 1 &amp; 3 \\
3 &amp; 1 &amp; 4 \\
4 &amp; 1 &amp; 5
\end{bmatrix}
\begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
0
\end{bmatrix}\]

<p>The solutions to this system of linear equations are:</p>

\[x = t \begin{bmatrix}
1 \\
1 \\
-1
\end{bmatrix}, \quad t \in \mathbb{R}\]

<p>Here, we can observe that $x = \begin{bmatrix} 0 \ 0 \ 0 \end{bmatrix}$ is one of the solutions, which means that $N(A)$ is a line passing through the origin.</p>

<p><br /><br /><br /></p>

<p>Finally, we can also write the null space as the orthogonal complement of the row space:</p>

\[N(A) = \left( R(A^T) \right)^\perp\]


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#linear-algebra" class="page__taxonomy-item p-category" rel="tag">Linear Algebra</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#mathematics" class="page__taxonomy-item p-category" rel="tag">Mathematics</a>
    
    </span>
  </p>


        

      </footer>

      

      

    </div>

    
  </article>

  
  
</div>
      
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 <a href="http://localhost:4000">Intuitive Maths</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>









  </body>
</html>

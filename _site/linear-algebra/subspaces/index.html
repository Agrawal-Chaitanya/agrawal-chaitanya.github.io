<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.1 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Understanding the Null Space ‚Äì From Geometry to ML Applications - Intuitive Maths</title>
<meta name="description" content="A website to learn maths intuitively">


  <meta name="author" content="Chaitanya Agrawal">
  


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Intuitive Maths">
<meta property="og:title" content="Understanding the Null Space ‚Äì From Geometry to ML Applications">
<meta property="og:url" content="http://localhost:4000/linear-algebra/subspaces/">


  <meta property="og:description" content="A website to learn maths intuitively">











  

  


<link rel="canonical" href="http://localhost:4000/linear-algebra/subspaces/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Intuitive Maths Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Intuitive Maths
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/"
                
                
              >Quick-Start Guide</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      packages: ['base', 'ams']
    },
    loader: {
      load: ['[tex]/ams']
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>








<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Understanding the Null Space ‚Äì From Geometry to ML Applications">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="http://localhost:4000/linear-algebra/subspaces/" itemprop="url">Understanding the Null Space ‚Äì From Geometry to ML Applications
</a>
          </h1>
          


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#lets-start-with-a-question">Let‚Äôs Start with a Question!</a></li><li><a href="#too-many-solutions-welcome-to-the-null-space">Too Many Solutions? Welcome to the Null Space</a></li><li><a href="#the-black-hole-of-matrices-what-gets-pulled-in-and-lost">The Black Hole of Matrices: What Gets Pulled In and Lost</a></li><li><a href="#zooming-into-the-math-what-exactly-is-the-null-space">Zooming into the Math: What Exactly Is the Null Space?</a></li><li><a href="#to-do">To Do</a></li></ul>
            </nav>
          </aside>
        
        <h2 id="lets-start-with-a-question">Let‚Äôs Start with a Question!</h2>

<ul>
  <li>What does it really mean for a matrix to ‚Äúsend‚Äù a vector to zero?</li>
  <li>Why should an ML engineer or a data scientist care about such a concept?</li>
</ul>

<p>If you‚Äôre diving into machine learning, understanding the <strong>null space</strong> is more than a textbook exercise. It‚Äôs key to understanding how models behave under the hood.</p>

<h2 id="too-many-solutions-welcome-to-the-null-space">Too Many Solutions? Welcome to the Null Space</h2>

<p>Let‚Äôs say you‚Äôre building a regression model to predict house prices and you are using the following features to train the model:</p>
<ul>
  <li><b>Feature 1</b> : Area of the house</li>
  <li><b>Feature 2</b> : Number of rooms</li>
  <li><b>Feature 3</b> : 2 * (Area of the house)</li>
</ul>

<p>You fit your model, get great accuracy and then realize: there are infinitely many sets of weights that yield the exact same predictions.</p>

<p>What‚Äôs going on here?</p>

<p>This is where the null space enters the picture. It represents the ‚Äúinvisible directions‚Äù (vectors) in your feature space, i.e., changes that don‚Äôt affect the output at all. In our example, this has happened because of the presence of multicollinearity.</p>

<h2 id="the-black-hole-of-matrices-what-gets-pulled-in-and-lost">The Black Hole of Matrices: What Gets Pulled In and Lost</h2>
<p>Let‚Äôs build some intuition before jumping into the math.</p>

<p>When we project a 3D object onto a 2D plane, all the information about the object‚Äôs depth (the component perpendicular to the plane) is lost and maps to zero in that dimension on the 2D plane. The vectors representing that ‚Äúdepth-only‚Äù information would be in the null space of the projection transformation.</p>

<p>A matrix transformation does something similar: it takes a vector and transforms it, but certain directions get completely wiped out. Those are the directions that belong to the null space.</p>

<p>So when we say a vector lies in the null space of a matrix $A$, we mean that applying $A$ to that vector turns it into the zero vector.</p>

<h2 id="zooming-into-the-math-what-exactly-is-the-null-space">Zooming into the Math: What Exactly Is the Null Space?</h2>

<p>Given matrix $A \in \mathbb{R}^{m \times n}$, we define the null space of $A$ as:</p>

\[N(A) = \{ x \in \mathbb{R}^n \mid Ax = 0 \}\]

<p>In simple terms:</p>
<ul>
  <li>It‚Äôs the set of all input vectors ùë• that the matrix ‚Äúkills‚Äù ‚Äî sends to the origin.</li>
  <li>It always forms a subspace of $\mathbb{R}^n$</li>
  <li>Its dimension tells you how many directions are ‚Äúinvisible‚Äù to your matrix.</li>
</ul>

<blockquote>
  <p>If the null space contains more than just the zero vector, your matrix is not full-rank, and your system of equations (or ML model) has multiple solutions.</p>
</blockquote>

<p>Let‚Äôs look at $N(A)$ from another perspective. We can represent its rows as row vectors:</p>

\[A = \begin{pmatrix}
a^T_{1} \\
a^T_{2} \\
a^T_{3} \\
\vdots \\
a^T_{m}
\end{pmatrix}\]

<p>where $a^T_i$ is the transpose of the $i^{\text{th}}$ row vector ($a_i$ is a column vector in $\mathbb{R}^n$).<br />
Now, when we compute $Ax$, we are essentially taking the dot product of each row of $A$ with the vector $x$:</p>

\[Ax = \begin{pmatrix}
a^T_{1} x \\
a^T_{2} x \\
a^T_{3} x \\
\vdots \\
a^T_{m} x
\end{pmatrix}
= 
\begin{pmatrix}
a_{1} \cdot x \\
a_{2} \cdot x \\
a_{3} \cdot x \\
\vdots \\
a_{m} \cdot x
\end{pmatrix}\]

<p>For $x$ to be in the $N(A)$, we must have $Ax = 0$. This means:</p>

\[\begin{pmatrix}
a_{1} \cdot x \\
a_{2} \cdot x \\
a_{3} \cdot x \\
\vdots \\
a_{m} \cdot x
\end{pmatrix}
=
\begin{pmatrix}
0 \\
0 \\
0 \\
\vdots \\
0
\end{pmatrix}\]

<p>This set of equations implies that for every row vector $a_i$ of $A$, the dot product with $x$ must be zero:</p>

\[a_i \cdot x = 0 \quad \text{for all } i = 1, 2, 3, \dots, m\]

<p>Since we know that the dot product of two vectors being zero means the vectors are orthogonal to each other,<br />
<strong>a vector $x$ is in the null space of $A$ if and only if $x$ is orthogonal to every row vector of matrix $A$.</strong></p>

<blockquote>
  <p>The row space of $A$, denoted as $\text{Row}(A)$, is the span of the row vectors of $A$. Since $x$ is orthogonal to each row vector, it must also be orthogonal to any linear combination of the row vectors. This means that $N(A)$ is orthogonal to the entire row space of $A$. We can write this as: <br />
\begin{gathered} 
 N(A) \perp \text{Row}(A) 
\end{gathered}</p>
</blockquote>

<p><b> Make the illustration of null space using following regression example as sub heading of ‚ÄúZooming into the math‚Äù </b></p>
<h2 id="to-do">To Do</h2>
<p>proof: please elaborate in detail - Let‚Äôs say you‚Äôre building a regression model. What if there are multiple equally good solutions?<br />
This usually means your data matrix has a <strong>null space</strong>‚Äîa subspace where changes in input don‚Äôt affect the output.</p>

<p>Suppose you‚Äôre solving a linear regression problem:</p>

<p>ùëã
ùõΩ
=
ùë¶
XŒ≤=y
ùëã
X is your design matrix (e.g., features).</p>

<p>ùõΩ
Œ≤ is the vector of coefficients you want to learn.</p>

<p>ùë¶
y is your output/target vector.</p>

<p>Now imagine that there is not just one 
ùõΩ
Œ≤ that gives you a good fit ‚Äî but infinitely many.</p>

<p>Why? Because the system has redundant information, and part of 
ùõΩ
Œ≤ can move freely without changing the outcome 
ùë¶
y.</p>

<p>This ‚Äúfreedom‚Äù is precisely what the null space captures.</p>

<p>üìê Geometric View
The null space of 
ùëã
X is the set of all vectors 
ùë£
v such that:</p>

<p>ùëã
ùë£
=
0
Xv=0
If 
ùë£
v is in the null space, and you have any solution 
ùõΩ
0
Œ≤ 
0
‚Äã
 , then:</p>

<p>ùõΩ
0
+
ùë£
Œ≤ 
0
‚Äã
 +v
is also a solution! Why?</p>

<p>Because:</p>

<p>ùëã
(
ùõΩ
0
+
ùë£
)
=
ùëã
ùõΩ
0
+
ùëã
ùë£
=
ùë¶
+
0
=
ùë¶
X(Œ≤ 
0
‚Äã
 +v)=XŒ≤ 
0
‚Äã
 +Xv=y+0=y
‚û°Ô∏è This means the null space contains the ‚Äúdirections‚Äù along which we can move without affecting predictions.</p>

<p>üß™ ML Interpretation
In practical ML:</p>

<p>The null space contains non-identifiable components of the model ‚Äî parts of the coefficient vector that you cannot learn uniquely from data.</p>

<p>If the null space is non-trivial (i.e., not just the zero vector), then:</p>

<p>There is collinearity (linearly dependent features).</p>

<p>The matrix 
ùëã
ùëá
ùëã
X 
T
 X is not invertible, so the normal equation doesn‚Äôt have a unique solution.
<strong>Example</strong>:</p>

\[Ax =
\begin{bmatrix}
1 &amp; 1 &amp; 2 \\
2 &amp; 1 &amp; 3 \\
3 &amp; 1 &amp; 4 \\
4 &amp; 1 &amp; 5
\end{bmatrix}
\begin{bmatrix}
x_{1} \\
x_{2} \\
x_{3}
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
0
\end{bmatrix}\]

<p>The solutions to this system of linear equations are:</p>

\[x = t \begin{bmatrix}
1 \\
1 \\
-1
\end{bmatrix}, \quad t \in \mathbb{R}\]

<p>Here, we can observe that $x = \begin{bmatrix} 0 \ 0 \ 0 \end{bmatrix}$ is one of the solutions, which means that $N(A)$ is a line passing through the origin.</p>

<p><br /><br /><br /></p>

<p>Finally, we can also write the null space as the orthogonal complement of the row space:</p>

\[N(A) = \left( R(A^T) \right)^\perp\]


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#linear-algebra" class="page__taxonomy-item p-category" rel="tag">Linear Algebra</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#mathematics" class="page__taxonomy-item p-category" rel="tag">Mathematics</a>
    
    </span>
  </p>


        

      </footer>

      

      

    </div>

    
  </article>

  
  
</div>
      
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 <a href="http://localhost:4000">Intuitive Maths</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>









  </body>
</html>
